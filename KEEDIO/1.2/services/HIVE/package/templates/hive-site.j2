<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<configuration>

<!-- WARNING!!! This file is provided for documentation purposes ONLY!     -->
<!-- WARNING!!! Any changes you make to this file will be ignored by Hive. -->
<!-- WARNING!!! You must make your changes in hive-site.xml instead.       -->


<!-- Hive Execution Parameters -->
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

<property>
  <name>mapred.job.tacker</name>
  <value>NONE</value>
</property>

<property>
  <name>hive.metastore.uris</name>
  <value>thrift://{{ hive_metastore_host }}:{{ hive_metastore_port }}</value>
  <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>{{ jdbc_connection }}</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>{{ jdbc_driver }}</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>{{ jdbc_username }}</value>
  <description>username to use against metastore database</description>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>{{ jdbc_password }}</value>
  <description>password to use against metastore database</description>
</property>

<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>false</value>
  <description>creates necessary schema on a startup if one doesn't exist. set this to false, after creating it once</description>
</property>

<property>
  <name>datanucleus.fixedDatastore</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.cache.level2.type</name>
  <value>None</value>
  <description>SOFT=soft reference based cache, WEAK=weak reference based cache.</description>
</property>

<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>{{ hive_metastore_warehouse }}</value>
  <description>location of default database for the warehouse</description>
</property>

<property>
  <name>hive.metastore.execute.setugi</name>
  <value>true</value>
  <description>In unsecure mode, setting this property to true will cause the metastore to execute DFS operations using the client's reported user and group permissions. Note that this property must be set on both the client and server sides. Further note that its best effort. If client sets its to true and server sets it to false, client setting will be ignored.</description>
</property>

<property>
  <name>hive.exec.compress.output</name>
  <value>false</value>
  <description> This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* </description>
</property>

<property>
  <name>hive.exec.compress.intermediate</name>
  <value>false</value>
  <description> This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from Hadoop config variables mapred.output.compress* </description>
</property>

<property>
  <name>hive.hwi.war.file</name>
  <value>lib/hive-hwi-@VERSION@.war</value>
  <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
</property>

<property>
  <name>hive.hwi.listen.host</name>
  <value>0.0.0.0</value>
  <description>This is the host address the Hive Web Interface will listen on</description>
</property>

<property>
  <name>hive.hwi.listen.port</name>
  <value>9999</value>
  <description>This is the port the Hive Web Interface will listen on</description>
</property>

<property>
  <name>hive.server2.thrift.http.port</name>
  <value>10001</value>
  <description>Port number when in HTTP mode.</description>
</property> 

<property>
  <name>hive.server2.thrift.http.path</name>
  <value>cliservice</value>
  <description>Path component of URL endpoint when in HTTP mode.</description>
</property> 

<property>
  <name>hive.metastore.sasl.enabled</name>
  <value>{{ security_enabled }}</value>
  <description>If true, the metastore Thrift interface will be secured with SASL. Clients must authenticate with Kerberos.</description>
</property>

<property>
  <name>hive.metastore.thrift.framed.transport.enabled</name>
  <value>false</value>
  <description>If true, the metastore Thrift interface will use TFramedTransport. When false (default) a standard TTransport is used.</description>
</property>

{% if security_enabled %}
<property>
  <name>hive.metastore.kerberos.keytab.file</name>
  <value>{{ hive_keytab }}</value>
  <description>The path to the Kerberos Keytab file containing the metastore Thrift server's service principal.</description>
</property>

<property>
  <name>hive.metastore.kerberos.principal</name>
  <value>{{ hive_metastore_principal }}</value>
  <description>The service principal for the metastore Thrift server. The special string _HOST will be replaced automatically with the correct host name.</description>
</property>
{% endif %}

<property>
  <name>hive.cluster.delegation.token.store.class</name>
  <value>org.apache.hadoop.hive.thrift.MemoryTokenStore</value>
  <description>The delegation token store implementation. Set to org.apache.hadoop.hive.thrift.ZooKeeperTokenStore for load-balanced cluster.</description>
</property>

<property>
  <name>hive.cluster.delegation.token.store.zookeeper.connectString</name>
  <value>localhost:2181</value>
  <description>The ZooKeeper token store connect string.</description>
</property>

<property>
  <name>hive.cluster.delegation.token.store.zookeeper.znode</name>
  <value>/hive/cluster/delegation</value>
  <description>The root path for token store data.</description>
</property>

<property>
  <name>hive.cluster.delegation.token.store.zookeeper.acl</name>
  <value>sasl:hive/host1@EXAMPLE.COM:cdrwa,sasl:hive/host2@EXAMPLE.COM:cdrwa</value>
  <description>ACL for token store entries. List comma separated all server principals for the cluster.</description>
</property>

<property>
  <name>hive.support.concurrency</name>
  <value>true</value>
  <description>Whether Hive supports concurrency or not. A ZooKeeper instance must be up and running for the default Hive lock manager to support read-write locks.</description>
</property>

<property>
  <name>hive.zookeeper.quorum</name>
  <value>{{ zookeeper_hosts }}</value>
  <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
</property>

<property>
  <name>hive.zookeeper.client.port</name>
  <value>2181</value>
  <description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
</property>

<property>
  <name>hive.zookeeper.namespace</name>
  <value>hive_zookeeper_namespace</value>
  <description>The parent node under which all ZooKeeper nodes are created.</description>
</property>

<property>
  <name>hive.zookeeper.clean.extra.nodes</name>
  <value>false</value>
  <description>Clean extra nodes at the end of the session.</description>
</property>

<property>
  <name>hive.exec.mode.local.auto</name>
  <value>false</value>
  <description> Let Hive determine whether to run in local mode automatically </description>
</property>

<property>
  <name>hive.security.authorization.enabled</name>
  <value>{{ security_enabled }}</value>
  <description>enable or disable the Hive client authorization</description>
</property>

<property>
  <name>hive.security.authorization.manager</name>
  <value>{% if security_enabled %}org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory{% else %}org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider{% endif %}</value>
  <description>The Hive client authorization manager class name.
  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.
  </description>
</property>

<property>
  <name>hive.security.metastore.authorization.manager</name>
  <value>{% if security_enabled %}org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider{% else %}org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider{% endif %}</value>
  <description>authorization manager class name to be used in the metastore for authorization.
  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider. 
  </description>
</property>

<property>
  <name>hive.security.authenticator.manager</name>
  <value>{% if security_enabled %}org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator{% else %}org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator{% endif %}</value>
  <description>hive client authenticator manager class name.
  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.</description>
</property>

<property>
  <name>hive.security.metastore.authenticator.manager</name>
  <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>
  <description>authenticator manager class name to be used in the metastore for authentication. 
  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.</description>
</property>

<property>
  <name>hive.security.authorization.createtable.owner.grants</name>
  <value>ALL</value>
  <description>the privileges automatically granted to the owner whenever a table gets created.
   An example like "select,drop" will grant select and drop privilege to the owner of the table</description>
</property>

<property>
  <name>hive.users.in.admin.role</name>
  <value>{{ hive_admin_users }}</value>
  <description>Comma separated list of users who are in admin role for bootstrapping.
    More users can be added in ADMIN role later.</description>
</property>

<property>
  <name>hive.warehouse.subdir.inherit.perms</name>
  <value>true</value>
  <description>Set this to true if the the table directories should inherit the
    permission of the warehouse or database directory instead of being created
    with the permissions derived from dfs umask</description>
</property>

<property>
  <name>hive.exec.orc.default.compress</name>
  <value>ZLIB</value>
  <description>
    Define the default compression codec for ORC file.
  </description>
</property>

<property>
  <name>hive.log4j.file</name>
  <value></value>
  <description>Hive log4j configuration file.
  If the property is not set, then logging will be initialized using hive-log4j.properties found on the classpath.
  If the property is set, the value must be a valid URI (java.net.URI, e.g. "file:///tmp/my-logging.properties"), which you can then extract a URL from and pass to PropertyConfigurator.configure(URL).</description>
</property>

<property>
  <name>hive.exec.log4j.file</name>
  <value></value>
  <description>Hive log4j configuration file for execution mode(sub command).
  If the property is not set, then logging will be initialized using hive-exec-log4j.properties found on the classpath.
  If the property is set, the value must be a valid URI (java.net.URI, e.g. "file:///tmp/my-logging.properties"), which you can then extract a URL from and pass to PropertyConfigurator.configure(URL).</description>
</property>

<property>
  <name>hive.server2.thrift.port</name>
  <value>{{ hive_server2_port }}</value>
  <description>Port number of HiveServer2 Thrift interface.
  Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT</description>
</property>

<property>
  <name>hive.server2.thrift.bind.host</name>
  <value>{{ hive_server2_host }}</value>
  <description>Bind host on which to run the HiveServer2 Thrift interface.
  Can be overridden by setting $HIVE_SERVER2_THRIFT_BIND_HOST</description>
</property>

<property>
  <name>hive.server2.authentication</name>
  <value>{% if security_enabled %}KERBEROS{% else %}NONE{% endif %}</value>
  <description>
    Client authentication types.
       NONE: no authentication check
       LDAP: LDAP/AD based authentication
       KERBEROS: Kerberos/GSSAPI authentication
       CUSTOM: Custom authentication provider
               (Use with property hive.server2.custom.authentication.class)
       PAM: Pluggable authentication module.
  </description>
</property>

{% if security_enabled %}
<property>
  <name>hive.server2.authentication.kerberos.principal</name>
  <value>{{ hive_server2_principal }}</value>
  <description>
    Kerberos server principal
  </description>
</property>

<property>
  <name>hive.server2.authentication.kerberos.keytab</name>
  <value>{{ hive_server2_keytab}}</value>
  <description>
    Kerberos keytab file for server principal
  </description>
</property>

<property>
  <name>hive.server2.authentication.spnego.principal</name>
  <value>{{ hive_server2_spnego_principal }}</value>
  <description>
    SPNego service principal, optional,
    typical value would look like HTTP/_HOST@EXAMPLE.COM
    SPNego service principal would be used by hiveserver2 when kerberos security is enabled
    and HTTP transport mode is used.
    This needs to be set only if SPNEGO is to be used in authentication.
  </description>
</property>

<property>
  <name>hive.server2.authentication.spnego.keytab</name>
  <value>{{ hive_server2_spnego_keytab }}</value>
  <description>
    keytab file for SPNego principal, optional,
    typical value would look like /etc/security/keytabs/spnego.service.keytab,
    This keytab would be used by hiveserver2 when kerberos security is enabled
    and HTTP transport mode is used.
    This needs to be set only if SPNEGO is to be used in authentication.
    SPNego authentication would be honored only if valid
    hive.server2.authentication.spnego.principal
    and
    hive.server2.authentication.spnego.keytab
    are specified.
  </description>
</property>
{% endif %}

<property>
  <name>hive.server2.enable.doAs</name>
  <value>false</value>
  <description>
   Setting this property to true will have HiveServer2 execute
    Hive operations as the user making the calls to it.
  </description>
</property>

<property>
  <name>hive.execution.engine</name>
  <value>mr</value>
  <description>
    Chooses execution engine. Options are mr (MapReduce, default) or Tez (Hadoop 2 only).
  </description>
</property>

<property>
  <name>hive.server2.thrift.sasl.qop</name>
  <value>auth</value>
  <description>Sasl QOP value; set it to one of the following values to enable higher levels of
     protection for HiveServer2 communication with clients.
      "auth" - authentication only (default)
      "auth-int" - authentication plus integrity protection
      "auth-conf" - authentication plus integrity and confidentiality protection
     Note that hadoop.rpc.protection being set to a higher level than HiveServer2 does not
     make sense in most situations. HiveServer2 ignores hadoop.rpc.protection in favor of
     hive.server2.thrift.sasl.qop.
     This is applicable only if HiveServer2 is configured to use Kerberos authentication.
 </description>
</property>
</configuration>
